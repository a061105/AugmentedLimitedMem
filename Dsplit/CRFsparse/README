CRFSparse is a proximal Quasi-Newton solver for minimizing L1-regularized conditional negative log-likelihood of Conditional Random Fields(CRF), in particular, for Sequence Labeling and Hierarchical Classification problems. 
This solver can also be used to optimize any differentiable convex loss functions with a L1-regularization, i.e., 
	f(w) = l(w) + \lambda |w|_1
where l(w) is a convex differentiable loss function and |w|_1 is the L1 norm of w. 
For users who want to implement their own models, please see the section, "Implementation for other problems".

===========================
	Compile
===========================
This package can be compiled by "make" in a unix system with g++.

===========================
	Usage
===========================
Training:
./train (options) [train_data] (model)
	
options:
	-p problem_type: (default 1)
		1 -- Sequence labeling (info file = feature template)
		2 -- Hierarchical classification (info file = taxonomy tree)
	-i info: (additional info specified in a file)
	-m max_iter: maximum_outer_iteration
	-l lambda: regularization coefficient (default 1.0)
	-e epsilon: stop criterion (default 1e-6)

Predicting:
Usage: ./predict (options) [test_data] [model] (prediction_result)

options:
	-p problem_type: (default 1)
		1 -- Sequence labeling (info file = feature template)
		2 -- Hierarchical classification (info file = taxonomy tree)
	-i info: (additional info specified in a file)

The "model" is the output model file of train, which includes the number of raw features in the first line and the indices of non-zero optimal weights and their corresponding values (sepearted by a colon ":") in the second line. 

===========================
	Example
===========================
Training for sequence labeling:
	$ ./train -p 1 -i ../example/seq_label/seq_label_feature_template ../example/seq_label/ocr.deg2.100

Predicting for sequence labeling:
	$  ./predict -p 1 -i ../example/seq_label/seq_label_feature_template ../example/seq_label/ocr.deg2.t49 model prediction_result	

Training for hierarchical classification:
	$ ./train -p 2 -l 0.001 -i ../example/taxonomy/cat_hier.txt ../example/taxonomy/train.hier.20 model.hier

Predicting for hierarchical classification:
	$ ./predict -p 2 -i ../example/taxonomy/cat_hier.txt ../example/taxonomy/test.hier.5 model.hier prediction_result

===========================
      File format
===========================

 Sequence labeling:

Data file format is similar with LIBSVM data format. In each line, the label is in the first place and then followed by index:feature pairs. Each sequence is separated by an empty line. The file, ocr.deg2.100, in example/seq_label/ is the first 10 sequences of the OCR data with degree 2 features.  

Feature template here is simplified from CRF++ format. seq_label_feature_template in example/seq_label/ gives an example of feature templates. The feature functions of this example are described in the paper. In particular, the first two lines denotes the possible labels. The following two lines are for unigram feature functions, where 0 denotes the feature function of combining EVERY feature of the current token and its label. -1 denotes a combination of EVERY feature of the current token and the label of its previous token. And 1 is similar. The last line is for bigram. A combination of the previous token's label and current token's label is included by default. 

 Hierarchical classification:

Data format is the same as LIBSVM. The file train.reindex.scale.10 in example/taxonomy generated by  first re-indexing and then svm-scaling from the first 10 instances of the train data of LSHTC1 TASK1.  For your own data, please use svm-scale in the LIBSVM package to scale the original data if necessary. 

The info file, taxnomony tree structure, is the same as cathier.txt for LSHTC1, see http://lshtc.iit.demokritos.gr/node/3 . 

===========================
 Implementation for other problems
===========================
This package contains a c++ class "problem", which users can use to optimize any L1-regularized differentiable convex loss function. Your specified problem class should be derived from "problem" class and implement/override the pure virtual functions in "problem" class. seq_label.h and taxonomy.h are two examples of derived problem class. 

The pure virtual functions in "problem" class are,

-- double fun() computes function value

-- void grad( vector<int>& act_set, vector<double>& g) computes the partial gradients for coordinates in act_set. 

-- void compute_fv_change(double* w_change, vector<int>& act_set, vector<pair<int,double> >& fv_change) computes factor value change due to a change of w, w_change. We will discuss factor values in the following subsection. w_change is a d by 1 array and only the coordinates in act_set have valid change. fv_change is the output, in which the first element in the pair is the index of changed factor and the second element is changing value. 

-- void update_fvalue(vector<pair<int,double> >& fv_change, double scalar) updates the factor values. new_fvalue = fvalue + scalar * fv_change. Note that only changed factors are updated. 

------- Factor Values ---------

For most log-linear models, the loss function l(w) has a form of two compositing functions,
	l(w | x) = J( h(w | x) ) = J(fv)  and  h( w_1 + w_2 | x) =  h(w_1 | x) + h(w_2 | x)     ------(1)
where fv = h(w|x) are called factor values, x is the data and c is a constant. 

Factors are intermediate variables for calculating gradients and function values from data and weights, i.e. to compute l(w | x), we first compute fv = h(w | x) and then compute J(fv), where J(fv) doesn't involve any data. For example, the factors in the hierarchical classification problem are inner product of <W_k, x>, where W_k is the weights for k-th class and x is an instance of data set. 

Here is how factor values speed up the line search procedure. In line search procedure, w_new = w + alpha * w_change, where alpha will probably try over multiple values and the function value is calculated in each try. If we record fv_change = h(w_change | x), we don't need to calculate h(w | x) in each try. Then in each try, we just calculate new_fv = fv + alpha * fv_change, and then obtain l(w_new) from J(new_fv). 

Here is a psudo-code for line search in the optimization,
************************************************
vector<pair<int,double> >& fv_change;
compute_fv_change(w_change, act_set, fv_change);
update_fvalue(fv_change, alpha);

while (criterion not attained)
{
	old_alpha = alpha;
	alpha = alpha/2;
	update_fvalue(fv_change, alpha-old_alpha);
	fun_value = fun();
}
************************************************

In many problems, like sequence labeling and hierarchical classification, not only the function value can be computed from factor values efficiently, but also gradients can be computed from factor values. Thus, introducing factor values will speed up the optimization, at least for the line search procedure. So please check if your model has a factor structure (1) and utilize it if it does. 

If your model doesn't have a factor structure, you can leave the factor-related functions, compute_fv_change and update_fvalue, empty. As long as fun() and grad() are computed correctly, this package still works.

===========================
Required software
===========================
This package depends on the Eigen library, http://eigen.tuxfamily.org. Eigen is distributed under the Mozilla Public License, version 2. It is already included in the 'lib' folder in this package.

===========================
Bug reports and comments are appreciated. Please send your email to: zhongkai at ices dot utexas dot edu

